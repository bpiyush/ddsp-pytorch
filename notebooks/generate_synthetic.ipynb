{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd89a12d-c4c1-42da-a8b5-9f23f3c9a31b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea3b360a-3480-4f7f-8bed-5aabd77d8ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torchaudio\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import librosa\n",
    "import PIL\n",
    "import matplotlib.pyplot as plt\n",
    "from omegaconf import OmegaConf\n",
    "from IPython.display import display, Markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abd83b40-bd21-421c-ac6e-2a60a2f50f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shared.utils as su"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa499ccf-e392-4a05-bd35-7c0ace4242f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(os.path.join(su.log.repo_path, \"train\"))\n",
    "from network.autoencoder.autoencoder import AutoEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e41e11-b19c-4da3-9b11-ae894ed17e74",
   "metadata": {},
   "source": [
    "### Load original samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93fab9d8-ddd3-4933-be1f-74b25dd983e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_root = su.paths.get_data_root_from_hostname()\n",
    "data_dir = os.path.join(\n",
    "    data_root, \"PouringLiquidsData\",\n",
    ")\n",
    "meta_dir = os.path.join(os.path.dirname(su.log.repo_path), \"PouringLiquidsData\")\n",
    "\n",
    "video_clip_dir = os.path.join(data_dir, \"resized_clips\")\n",
    "audio_clip_dir = os.path.join(data_dir, \"resized_clips_wav\")\n",
    "# First frame of the video to get a sense of the container\n",
    "frame_dir = os.path.join(data_dir, \"first_frames\")\n",
    "annot_dir = os.path.join(meta_dir, \"annotations\")\n",
    "\n",
    "# Load side information: containers\n",
    "container_path = os.path.join(\n",
    "    meta_dir, \"annotations/containers.yaml\",\n",
    ")\n",
    "assert os.path.exists(container_path)\n",
    "containers = su.io.load_yml(container_path)\n",
    "\n",
    "# Load CSV\n",
    "csv_path = os.path.join(annot_dir, f\"localisation.csv\")\n",
    "df = pd.read_csv(csv_path)\n",
    "print(\" [:::] Shape of CSV (in original form): \", df.shape)\n",
    "\n",
    "# Update CSV with container information (optional)\n",
    "update_with_container_info = True\n",
    "if update_with_container_info:\n",
    "    rows = []\n",
    "    for row in df.iterrows():\n",
    "        row = row[1].to_dict()\n",
    "        row.update(containers[row[\"container_id\"]])\n",
    "        rows.append(row)\n",
    "    df = pd.DataFrame(rows)\n",
    "\n",
    "# Update item_id\n",
    "df[\"item_id\"] = df.apply(\n",
    "    lambda d: f\"{d['video_id']}_{d['start_time']:.1f}_{d['end_time']:.1f}\",\n",
    "    axis=1,\n",
    ")\n",
    "# Update video clip path\n",
    "df[\"video_clip_path\"] = df[\"item_id\"].apply(\n",
    "    lambda d: os.path.join(\n",
    "        video_clip_dir, f\"{d}.mp4\"\n",
    "    )\n",
    ")\n",
    "df = df[df[\"video_clip_path\"].apply(os.path.exists)]\n",
    "print(\" [:::] Shape of CSV with available video: \", df.shape)\n",
    "# Update audio clip path\n",
    "df[\"audio_clip_path\"] = df[\"item_id\"].apply(\n",
    "    lambda d: os.path.join(\n",
    "        audio_clip_dir, f\"{d}.wav\"\n",
    "    )\n",
    ")\n",
    "df = df[df[\"audio_clip_path\"].apply(os.path.exists)]\n",
    "print(\" [:::] Shape of CSV with available audio: \", df.shape)\n",
    "\n",
    "# Update first frame path\n",
    "df[\"first_frame_path\"] = df[\"video_id\"].apply(\n",
    "    lambda d: os.path.join(\n",
    "        frame_dir, f\"{d}.png\"\n",
    "    )\n",
    ")\n",
    "df = df[df[\"first_frame_path\"].apply(os.path.exists)]\n",
    "print(\" [:::] Shape of CSV with available frames: \", df.shape)\n",
    "\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dade803-6a24-4a83-93ae-024cf792e145",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter only cylindrical containers\n",
    "df = df[df[\"shape\"].isin([\"cylindrical\"])]\n",
    "print(\" [:::] Shape of CSV with only cylindrical containers: \", df.shape)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fbe30b2-a405-4da1-a61e-d297a93fbc28",
   "metadata": {},
   "source": [
    "### Load Autoencoder model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e27f2ae3-5180-4710-a21e-b3e19ef5c025",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load config\n",
    "config = os.path.join(su.log.repo_path, \"configs/pld_80.yaml\")\n",
    "config = OmegaConf.load(config)\n",
    "\n",
    "# Load model\n",
    "net = AutoEncoder(config)\n",
    "\n",
    "# Load trained checkpoint\n",
    "ckpt_dir = \"/work/piyush/experiments/ddsp-pytorch/pld_80/checkpoints\"\n",
    "ckpt_id = \"200131.pth-100000\"\n",
    "ckpt_path = os.path.join(ckpt_dir, ckpt_id)\n",
    "assert os.path.exists(ckpt_path), f\"Checkpoints does not exist at {ckpt_path}\"\n",
    "print(\" [:::] Checkpoint path: \", ckpt_path)\n",
    "msg = net.load_state_dict(torch.load(ckpt_path))\n",
    "print(\" [:::] Loaded checkpoint: \", msg)\n",
    "\n",
    "# Important: set in eval mode\n",
    "net.eval()\n",
    "\n",
    "# Move to device\n",
    "net = net.to(device)\n",
    "\n",
    "su.misc.num_params(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63e70c11-df4e-4a88-964d-6157e9dc9509",
   "metadata": {},
   "source": [
    "### Generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ceed12-4220-423a-9caa-fdae45404ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sr = 16000\n",
    "stft = dict(n_fft=512, hop_length=256, n_mels=64)\n",
    "margin = 8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "751bed94-e6f9-4620-975a-ea6b9d37383a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load original audio to compute loudness\n",
    "def load_audio(audio_path, sample_rate):\n",
    "\n",
    "    # Load\n",
    "    y, true_sample_rate = torchaudio.load(audio_path)\n",
    "\n",
    "    # Resample if sampling rate is not equal to model's\n",
    "    if true_sample_rate != sample_rate:\n",
    "        resampler = torchaudio.transforms.Resample(true_sample_rate, sample_rate)\n",
    "        y = resampler(y)\n",
    "\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1694c1de-b374-4078-b8f6-3874484c262c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def waveform_to_logmelspec(y, sr, n_fft, hop_length, n_mels, fmin=0, fmax=sr//2):\n",
    "    \"\"\"Converts a waveform (torch.Tensor) to log-mel-spectrogram.\"\"\"\n",
    "\n",
    "    if len(y.shape) == 2:\n",
    "        y = y.squeeze(0)\n",
    "    y = y.cpu().numpy()\n",
    "    \n",
    "    S = librosa.feature.melspectrogram(\n",
    "        y=y, sr=sr, n_fft=n_fft, hop_length=hop_length, n_mels=n_mels, fmax=fmax, fmin=fmin\n",
    "    )\n",
    "    S = librosa.power_to_db(S, ref=np.max)\n",
    "\n",
    "    return S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0735b56-ea6a-49b4-b1db-386c7175d7f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = np.random.choice(len(df))\n",
    "print(\"Index: \", i)\n",
    "row = df.iloc[i].to_dict()\n",
    "\n",
    "audio_path = row[\"audio_clip_path\"]\n",
    "\n",
    "# Load audio\n",
    "y = load_audio(audio_path, config.sample_rate)\n",
    "\n",
    "# Get logmelspectrogram\n",
    "S = waveform_to_logmelspec(y=y, sr=sr, **stft)\n",
    "\n",
    "# Get first frame (to show container)\n",
    "first_frame = PIL.Image.open(row[\"first_frame_path\"])\n",
    "\n",
    "# Get measurements & duration\n",
    "measurements = row[\"measurements\"]\n",
    "h_true = measurements[\"net_height\"]\n",
    "r_true = 0.25 * (measurements[\"diameter_bottom\"] + measurements[\"diameter_top\"])\n",
    "duration_true = row[\"end_time\"] - row[\"start_time\"]\n",
    "\n",
    "y.shape, S.shape, first_frame.size, r_true, h_true, duration_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6efc90f-7c01-499f-96b2-ebe6be3f91aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_true_example(first_frame, S, r_true=None, h_true=None, duration_true=None, figsize=(11, 3.4), title_prefix=\"\"):\n",
    "    fig = plt.figure(figsize=figsize, constrained_layout=True)\n",
    "    gs = fig.add_gridspec(1, 5)\n",
    "    ax = fig.add_subplot(gs[0])\n",
    "    ax.imshow(first_frame)\n",
    "    r = lambda x: np.round(x, 2)\n",
    "    \n",
    "    ax = fig.add_subplot(gs[1:])\n",
    "    img = librosa.display.specshow(\n",
    "        S, x_axis='time', y_axis='mel', sr=sr, fmax=8000, ax=ax,\n",
    "    )\n",
    "    # fig.colorbar(img, ax=ax, format='%+2.0f dB')\n",
    "\n",
    "    title = f\"[{title_prefix}]     Radius: {r(r_true)} (cms) | Height: {r(h_true)} (cms) | Duration: {r(duration_true)} (s)\"\n",
    "    plt.suptitle(title, y=0.95)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    fig.colorbar(img, ax=ax, format='%+2.0f dB')\n",
    "    plt.show()\n",
    "\n",
    "# show_true_example(first_frame, S, r_true, h_true, duration_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d44e91d-604d-4887-99cf-a0582e38bc78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get loudness from a real audio\n",
    "with torch.no_grad():\n",
    "    loudness = net.encoder.loudness_extractor({\"audio\": y.to(device)})\n",
    "\n",
    "num_frames = loudness.shape[1]\n",
    "\n",
    "loudness.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e25b8dc2-6052-4501-89a3-04b5b0e30ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define constants\n",
    "c = 340 * 100 # cms/sec\n",
    "beta = 0.62 # 1/sec\n",
    "F_max = (sr // 2) / 2.\n",
    "F_min = 0\n",
    "radius_range = [1., 5.] # cm\n",
    "height_range = [5., 20.] # cm\n",
    "duration_range = [5., 15.] # sec\n",
    "\n",
    "\n",
    "# Define the frequency bins (typically, 257 bins)\n",
    "frequencies = librosa.fft_frequencies(\n",
    "    sr=sr, n_fft=stft[\"n_fft\"],\n",
    ")\n",
    "num_freqs = len(frequencies)\n",
    "\n",
    "\n",
    "# Define functions\n",
    "def compute_length_of_air_column(T, duration, height, b=0.01):\n",
    "    \"\"\"\n",
    "    Randomly chooses a l(t) curve satisfying the two point equations.\n",
    "    By default approximates a linear curve.\n",
    "    \"\"\"\n",
    "    if b is None:\n",
    "        # Choose randomly\n",
    "        b = np.random.uniform(-1, 1.)\n",
    "        # Ensure that b is non-zero\n",
    "        if b == 0:\n",
    "            b = 0.1\n",
    "    else:\n",
    "        # Use b as is given\n",
    "        pass\n",
    "    L = height * ( (1 - np.exp(b * (duration - T))) / (1 - np.exp(b * duration)) )\n",
    "    return L, b\n",
    "\n",
    "\n",
    "def compute_fundamental_frequencies(duration, height, radius, b=0.01, num_evals=100):\n",
    "    \"\"\"Computes the F0 given duration and container parameters.\"\"\"\n",
    "\n",
    "    global sr, stft\n",
    "\n",
    "    # Sample timestamps T\n",
    "    num_frames = librosa.time_to_frames(\n",
    "        [duration],\n",
    "        sr=sr,\n",
    "        hop_length=stft[\"hop_length\"],\n",
    "        n_fft=stft[\"n_fft\"],\n",
    "    )[0]\n",
    "    T = np.linspace(0, duration, num_evals)\n",
    "\n",
    "    # Compute length of air column\n",
    "    L, b = compute_length_of_air_column(T, duration, height, b)\n",
    "    \n",
    "    # Compute fundamental frequency curve\n",
    "    F0 = (0.25 * c) * (1. / (L + (beta * radius)))\n",
    "\n",
    "    return T, L, F0, b\n",
    "\n",
    "\n",
    "def compute_dynamics(measurements, duration, num_evals=100):\n",
    "    \"\"\"Computes theoretical estimate of l(t) and f(t) for a cylinder.\"\"\"\n",
    "\n",
    "    h = measurements[\"net_height\"]\n",
    "    r_bot = measurements[\"diameter_bottom\"] / 2.\n",
    "    r_top = measurements[\"diameter_top\"] / 2.\n",
    "    r = (r_bot + r_top) / 2.\n",
    "\n",
    "    # Create a physics vector\n",
    "    b = 0.01\n",
    "    physical_params = np.array([h, r, duration, b])\n",
    "\n",
    "    T, L, F0, b = compute_fundamental_frequencies(duration, h, r, b=b, num_evals=num_evals)\n",
    "\n",
    "    return T, L, F0, duration, physical_params\n",
    "\n",
    "\n",
    "def show_latents(T, L, F0, physical_params):\n",
    "    \"\"\"Plots the latent parameters like length of air column and frequency.\"\"\"\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 3))\n",
    "    ax = axes[0]\n",
    "    ax.grid(alpha=0.4)\n",
    "    ax.set_xlabel(\"Time (s)\")\n",
    "    ax.set_ylabel(\"Length of air columns (cms)\")\n",
    "    ax.plot(T, L, \"--\", color=\"blue\", linewidth=2.)\n",
    "    ax.set_xlim(0, T[-1])\n",
    "    ax = axes[1]\n",
    "    ax.grid(alpha=0.4)\n",
    "    ax.set_xlabel(\"Time (s)\")\n",
    "    ax.set_ylabel(\"Frequency (Hz)\")\n",
    "    ax.plot(T, F0, \"--\", color=\"orange\", linewidth=2.)\n",
    "    ax.set_xlim(0, T[-1])\n",
    "\n",
    "    height, radius, duration, b = physical_params\n",
    "    r = lambda x: np.round(x, 3)\n",
    "    title = \"$\\\\Theta = \\{ R = %s, \\  H = %s, \\ D = %s, \\ b=%s \\}$\" % (r(radius), r(height), r(duration), r(b))\n",
    "    plt.suptitle(title, y=0.99, fontsize=17)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b33bf01-0675-498c-ab81-1357c35aa70f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a container with random measurements\n",
    "radius_ = np.random.uniform(*radius_range)\n",
    "height_ = np.random.uniform(*height_range)\n",
    "duration_ = np.random.uniform(*duration_range)\n",
    "\n",
    "# Define measurements of the synthetic container\n",
    "measurements_ = dict(\n",
    "    diameter_top=2 * radius_,\n",
    "    diameter_bottom=2 * radius_,\n",
    "    net_height=height_,\n",
    ")\n",
    "\n",
    "# Compute dynamics of pouring water in this container\n",
    "T, L, F0, duration, physical_params = compute_dynamics(\n",
    "    measurements_, duration_, num_evals=num_frames,\n",
    ")\n",
    "\n",
    "# Plot the latents (of the dynamic system)\n",
    "show_latents(T, L, F0, physical_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e67fa31-63af-4ced-a33c-1eeade0d08ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic audio\n",
    "\n",
    "batch = {\"f0\": torch.from_numpy(F0).to(loudness.device).float().unsqueeze(0), \"loudness\": loudness}\n",
    "latent = net.decoder(batch)\n",
    "harmonic = net.harmonic_oscillator(latent)\n",
    "noise = net.filtered_noise(latent)\n",
    "audio_synth = harmonic + noise[:, : harmonic.shape[-1]]\n",
    "\n",
    "# Increase volume\n",
    "audio_synth = audio_synth * 1\n",
    "audio = dict(\n",
    "    harmonic=harmonic, noise=noise, audio_synth=audio_synth,\n",
    ")\n",
    "audio[\"audio_reverb\"] = net.reverb(audio)\n",
    "audio[\"a\"] = latent[\"a\"]\n",
    "audio[\"c\"] = latent[\"c\"]\n",
    "# dereverb = audio[\"audio_synth\"].cpu()\n",
    "\n",
    "y_generated_dereverb = audio[\"audio_synth\"].cpu()\n",
    "y_generated_reverb = audio[\"audio_reverb\"].cpu()\n",
    "\n",
    "y.shape, y_generated_reverb.shape, y_generated_dereverb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b53d5c40-c9d8-4031-b989-c169088d8c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show original example\n",
    "su.log.print_update(\"Showing original example \", pos=\"left\", color=\"blue\")\n",
    "\n",
    "show_true_example(first_frame, S, r_true, h_true, duration_true, title_prefix=\"Original\")\n",
    "su.visualize.show_single_audio(data=y.cpu().numpy()[0], rate=sr)\n",
    "\n",
    "display(Markdown('---'))\n",
    "\n",
    "\n",
    "# Show generated example\n",
    "su.log.print_update(\"Showing generated example \", pos=\"left\", color=\"blue\")\n",
    "empty_image = PIL.Image.new('RGB', first_frame.size)\n",
    "\n",
    "y_gen = y_generated_dereverb\n",
    "S_gen = waveform_to_logmelspec(y=y_gen.detach(), sr=sr, **stft)\n",
    "show_true_example(empty_image, S_gen, radius_, height_, duration_, title_prefix=\"Generated\")\n",
    "su.visualize.show_single_audio(data=y_gen.detach().cpu().numpy()[0], rate=sr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f81cf83b-8517-4481-8d51-01e568a53c3f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a4844ee-a04c-4571-90a7-d7d37353a854",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac96ffe-13ea-4d33-a4c7-be8fe236aef7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "23b7503f-65ea-4617-9e36-0328e1020045",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Old code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b534710-ffd9-4ddd-9fe5-41754fcef459",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_generated_dereverb = audio[\"audio_synth\"].cpu()\n",
    "output_id = \"sample_synthetic_dereverb\"\n",
    "torchaudio.save(\n",
    "    os.path.splitext(output_id)[0] + \"_synth.wav\", y_generated_dereverb, sample_rate=config.sample_rate\n",
    ")\n",
    "\n",
    "y_generated_reverb = audio[\"audio_reverb\"].cpu()\n",
    "output_id = \"sample_synthetic_reverb\"\n",
    "torchaudio.save(\n",
    "    os.path.splitext(output_id)[0] + \"_synth.wav\", y_generated_reverb, sample_rate=config.sample_rate\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b0b0103-9610-4230-b9c6-4f1b8ae15eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "\n",
    "def greet(name, intensity):\n",
    "    return \"Hello, \" + name + \"!\" * int(intensity)\n",
    "\n",
    "demo = gr.Interface(\n",
    "    fn=greet,\n",
    "    inputs=[\"text\", \"slider\"],\n",
    "    outputs=[\"text\"],\n",
    ")\n",
    "\n",
    "demo.launch(server_name=\"localhost\", inline=True, share=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2368e109-3605-4934-9f51-08b248baed83",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
